{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Choosing DE Daten KI\n",
    "\n",
    "Rodar isso na máquina virtual\n",
    "Revisar os paths dos arquivos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando tudo que já está fixo:\n",
    "\n",
    "# Data processing\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Topic model\n",
    "import umap\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "# Countvectorizer (para stopwords)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## For UMAP Analisys:\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# para montar as combinacoes:\n",
    "from itertools import product\n",
    "\n",
    "# Para a matrix \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Topic Coherence:\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "## Functions to clean data:\n",
    "\n",
    "def replace_char(x):\n",
    "  return re.sub('[^a-zA-Z\\süöäß]', '', x)\n",
    "\n",
    "\n",
    "def clean_corpus(s):\n",
    "    s = s.apply(lambda x: x.lower() if isinstance(x, str) else x)     # tolower:\n",
    "    idx = s.apply(lambda x: isinstance(x, str))                       # Criar um índice booleano para filtrar os valores não-texto\n",
    "    s = s.drop(idx[~idx].index)                                       # Remover os valores não-texto\n",
    "    s = s.apply(replace_char)                                         # remover as partes de palavras que nao sao caracteres ou espacos\n",
    "    idx = s.apply(lambda x: len(x) < 2)                               # Criar um índice booleano para filtrar os elementos com comprimento menor que 2\n",
    "    s = s.drop(idx[idx].index)                                        # Remover os elementos com comprimento menor que 2\n",
    "    idx = s.apply(lambda x: x == '')                                  # Criar um índice booleano para filtrar os elementos vazios\n",
    "    s = s.drop(idx[idx].index)                                        # Remover os elementos vazios\n",
    "    \n",
    "    return s\n",
    "\n",
    "def calculuate_coherence_score(topic_model):  \n",
    "  #variable\n",
    "  topic_words = topic_words = [[words for words, _ in topic_model.get_topic(topic) if words!=''] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "  vectorizer = topic_model.vectorizer_model\n",
    "  tokenizer = vectorizer.build_tokenizer()\n",
    "  #dictionary\n",
    "  tokens = [doc.split() for doc in docs]\n",
    "  dictionary = corpora.Dictionary(tokens)\n",
    "  corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "\n",
    "  coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "  coherence = coherence_model.get_coherence()\n",
    "\n",
    "  \n",
    "  return coherence\n",
    "\n",
    "\n",
    "# creating parameters combinations\n",
    "\n",
    "n = [5, 7, 10, 20]  # n_neighbors\n",
    "d = [0.0, 0.1, 0.25]   # min_dist\n",
    "c = [10, 15, 20, 25, 30] #min_cluster\n",
    "s = [c_i/4 for c_i in c]  # min_sample\n",
    "\n",
    "combinations = list(product(n, d, c, s))\n",
    "\n",
    "combinations = [comb for comb in combinations if comb[3] == comb[2]/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#################### CÉLULA PARA SER MODIFICADA ##########################\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_original = pd.read_json(\"C:\\\\Users\\\\opc\\\\Documents\\\\Python_Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\de_concatenado.json\")\n",
    "\n",
    "\n",
    "# German Bund stopwords que eu alterei:\n",
    "# Carregue a lista a partir do arquivo\n",
    "with open(\"C:\\\\Users\\\\opc\\\\Documents\\\\Python_Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\german_bund_stopwords.json\", \"r\") as f:\n",
    "    german_stopwords_alterada = json.load(f) \n",
    "\n",
    "\n",
    "# 1. Dados com os quais vamos trabalhar:\n",
    "#daten KI\n",
    "# 'nationale_datenstrategie', 'nationale_ki_strategie_addendum', 'nationale_ki_strategie'\n",
    "df_work = df_original.query(\"documento == 'nationale_datenstrategie' | documento == 'nationale_ki_strategie_addendum' | documento == 'nationale_ki_strategie' \")\n",
    "\n",
    "# nome dos dados:\n",
    "tinicio = 'de_daten_ki_'\n",
    "\n",
    "titulo_df_avg_similarity = 'df_avg_similarity_coerence_coeficient_' + tinicio + '.csv'    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos ver primeiro o que o daten_ki traz:\n",
    "\n",
    "docs = df_work['corpus']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "### início do loop: rodando todas as combinacoes de parâmetros para fazer o meu modelo:\n",
    "\n",
    "df = pd.DataFrame({'parameters': [],'n_neighbors': [], 'min_dist': [], 'min_cluster' : [], \n",
    "                      'min_sample': [], 'topic_coherence': [], 'nr_topics': [], 'avg_similarity' : [], 'topics': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    n_neighbors = comb[0]\n",
    "    min_dist = comb[1]\n",
    "    min_cluster = comb[2]\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, \n",
    "                        umap_model=umap_model, \n",
    "                        hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Matrix de similaridade:\n",
    "\n",
    "    matrixA = cosine_similarity(np.array(topic_model.topic_embeddings_)[1:, :])\n",
    "    avg_similarity = np.average(matrixA)\n",
    "    labels = (topic_model.get_topic_info().sort_values(\"Topic\", ascending=True).Name)[1:]\n",
    "    nr_topics = len(labels)\n",
    "    labels = labels.str.cat(sep='; ')\n",
    "    title = tinicio + 'n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "    \n",
    "    #coherence score:\n",
    "    cs = calculuate_coherence_score(topic_model)\n",
    "\n",
    "    df_temp = pd.DataFrame({'parameters': [title],\n",
    "                            'n_neighbors': [n_neighbors],\n",
    "                            'min_dist': [min_dist],\n",
    "                            'min_cluster' : [min_cluster],\n",
    "                            'min_sample': [min_sample], \n",
    "                            'topic_coherence': [cs],\n",
    "                            'nr_topics': [nr_topics],\n",
    "                            'avg_similarity' : [avg_similarity],\n",
    "                            'topics': [labels]})\n",
    "    \n",
    "\n",
    "    pd.DataFrame({'parameters': [], \n",
    "                       'nr_topics': [], 'avg_similarity' : [], 'topics': []})\n",
    "\n",
    "\n",
    "    df = pd.concat([df, df_temp])\n",
    "\n",
    "df.to_csv(titulo_df_avg_similarity, sep = ';', index = False, )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Olhando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reabrindo o arquivo, caso eu tenha fechado\n",
    "#df = pd.read_csv(titulo_df_avg_similarity, sep=';')\n",
    "\n",
    "df1 = df\n",
    "df1 = df1.sort_values('avg_similarity')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a dispersao:\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# criando o gráfico de dispersão\n",
    "sns.scatterplot(data=df1, x='avg_similarity', y='topic_coherence', hue='nr_topics')\n",
    "\n",
    "# adicionando labels de eixo\n",
    "plt.xlabel('avg_similarity')\n",
    "plt.ylabel('topic_coherence')\n",
    "\n",
    "# adicionando título\n",
    "titulo_do_plot = tinicio + \": topic coherence X avg similarity\"\n",
    "plt.title(titulo_do_plot)\n",
    "\n",
    "# exibindo o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# Modificar aqui também!\n",
    "\n",
    "# reabrindo o arquivo, caso eu tenha fechado\n",
    "df1 = pd.read_csv(titulo_df_avg_similarity, sep=';')\n",
    "\n",
    "df1 = df1.query('topic_coherence > 0.38 & avg_similarity < 0.775')\n",
    "df1 = df1.reset_index()\n",
    "\n",
    "combinations = []\n",
    "\n",
    "for i in range(0,len(df1)):\n",
    "    n = df1.iloc[i, 2]  # linha coluna\n",
    "    d = df1.iloc[i, 3]\n",
    "    c = int(df1.iloc[i, 4])\n",
    "    s = int(df1.iloc[i, 5])\n",
    "    \n",
    "    com = [(n, d, c, s)]\n",
    "\n",
    "    combinations.extend(com)\n",
    "\n",
    "for item in combinations:\n",
    "    for element in item:\n",
    "        if type(element) == np.float64:\n",
    "            element = float(element)\n",
    "\n",
    "for comb in combinations:\n",
    "    print(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados\n",
    "\n",
    "docs = df_work['corpus']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "### início do loop:\n",
    "\n",
    "df2 = pd.DataFrame({'Document': [],'Topic': [], 'Name': [], 'Top_n_words' : [], 'Probability': [], 'Representative_document': [], 'model': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    print(comb)\n",
    "\n",
    "\n",
    "    n_neighbors = int(comb[0])\n",
    "    min_dist = float(comb[1])\n",
    "    min_cluster = int(comb[2])\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Dataframe with representative examples:\n",
    "\n",
    "    df_temp = topic_model.get_document_info(docs)\n",
    "    df_temp = df_temp.query('Representative_document == True')\n",
    "\n",
    "    #confirmando o modelo desse df\n",
    "    position = combinations.index(comb)\n",
    "    position = str(position)\n",
    "\n",
    "    #add modelo\n",
    "    df_temp['model'] =  str(comb)\n",
    "\n",
    "    df2 = pd.concat([df2, df_temp], axis=0)\n",
    "\n",
    "        ##### Visualizacao dos resultados\n",
    "\n",
    "    title = tinicio + '_n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "\n",
    "    fig_1 = topic_model.visualize_heatmap()\n",
    "    fig_1.update_layout(title = title)\n",
    "\n",
    "    ############################################Visualizacao da distribuicao\n",
    "\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")   #Suitable Score Functions: cosine\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    # Train BERTopic\n",
    "    topic_model = BERTopic(language = \"german\", \n",
    "                            umap_model=umap_model,\n",
    "                            vectorizer_model=vectorizer_model,\n",
    "                            hdbscan_model=hdbscan_model).fit(docs, embeddings)\n",
    "\n",
    "\n",
    "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    fig_2 = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "    fig_2.update_layout(title = title)\n",
    "\n",
    "    title = re.sub(r'\\s', '_', title) + '.html'\n",
    "\n",
    "    with open(title, 'a') as f:\n",
    "        f.write(fig_1.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write(fig_2.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "#Fora do loop:\n",
    "\n",
    "tituloex_models = 'df_examples_models_' + tinicio + '.csv'\n",
    "df2.to_csv(tituloex_models, sep = ';', index = False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc948d7f5534a86a8885c7bcf9c4eb48b42c573fd8ef9d73032ed45fcd97e814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
