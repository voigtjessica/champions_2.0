{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter settings - EU Daten und KI\n",
    "\n",
    "Rodar a seguinte rotina: For loop com as matrix de similaridade para ```umap::n_neighbors [5, 7, 10, 20]```, ```umap::min_dist [0.0, 0.1, 0.25]``` , and ```hdbscan::min_cluster_size``` and ```min_samples {10: 2.5, 15: 3.75, 20: 5.0, 25: 6.25, 30: 7.5}```  . Esses parametros baseiam-se nas documentacoes do BERTopic , HDBSCAN e UMAP . Decidimos permanecer com os valores default para outros importantes parametros como ```umap::n_components (5)```,  ```CountVectorizer::n_gram_range (1,1)```, ```BERTopic::min_topic_size (10)``` e automático ```BERTopic::nr_topics``` . Vamos olhar o AVG similarity e o número de tópicos para cada uma dessas combinacoes, a partir disso vamos decidir quais visualizacoes vamos fazer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\opc\\Documents\\Python_Scripts\\champions_2.0\\python3_9\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importando tudo que já está fixo:\n",
    "\n",
    "# Data processing\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Topic model\n",
    "import umap\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "# Countvectorizer (para stopwords)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# German stopwords que eu alterei:\n",
    "# Carregue a lista a partir do arquivo\n",
    "with open(\"C:\\\\Users\\\\opc\\\\Documents\\\\Python_Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\german_stopwords.json\", \"r\") as f:\n",
    "    german_stopwords_alterada = json.load(f) \n",
    "\n",
    "\n",
    "## For UMAP Analisys:\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# para montar as combinacoes:\n",
    "from itertools import product\n",
    "\n",
    "# Para a matrix \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Topic Coherence:\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "## Functions to clean data:\n",
    "\n",
    "def replace_char(x):\n",
    "  return re.sub('[^a-zA-Z\\süöäß]', '', x)\n",
    "\n",
    "\n",
    "def clean_corpus(s):\n",
    "    s = s.apply(lambda x: x.lower() if isinstance(x, str) else x)     # tolower:\n",
    "    idx = s.apply(lambda x: isinstance(x, str))                       # Criar um índice booleano para filtrar os valores não-texto\n",
    "    s = s.drop(idx[~idx].index)                                       # Remover os valores não-texto\n",
    "    s = s.apply(replace_char)                                         # remover as partes de palavras que nao sao caracteres ou espacos\n",
    "    idx = s.apply(lambda x: len(x) < 2)                               # Criar um índice booleano para filtrar os elementos com comprimento menor que 2\n",
    "    s = s.drop(idx[idx].index)                                        # Remover os elementos com comprimento menor que 2\n",
    "    idx = s.apply(lambda x: x == '')                                  # Criar um índice booleano para filtrar os elementos vazios\n",
    "    s = s.drop(idx[idx].index)                                        # Remover os elementos vazios\n",
    "    \n",
    "    return s\n",
    "\n",
    "def calculuate_coherence_score(topic_model):  \n",
    "  #variable\n",
    "  topic_words = topic_words = [[words for words, _ in topic_model.get_topic(topic) if words!=''] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "  vectorizer = topic_model.vectorizer_model\n",
    "  tokenizer = vectorizer.build_tokenizer()\n",
    "  #dictionary\n",
    "  tokens = [doc.split() for doc in docs]\n",
    "  dictionary = corpora.Dictionary(tokens)\n",
    "  corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "\n",
    "  coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "  coherence = coherence_model.get_coherence()\n",
    "\n",
    "  \n",
    "  return coherence\n",
    "\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_data_ki = pd.read_csv(\"C:\\\\Users\\\\opc\\\\Documents\\\\Python_Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\df_data_ki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating parameters combinations\n",
    "\n",
    "n = [5, 7, 10, 20]  # n_neighbors\n",
    "d = [0.0, 0.1, 0.25]   # min_dist\n",
    "c = [10, 15, 20, 25, 30] #min_cluster\n",
    "s = [c_i/4 for c_i in c]  # min_sample\n",
    "\n",
    "combinations1 = list(product(n, d, c, s))\n",
    "\n",
    "combinations1 = [comb for comb in combinations1 if comb[3] == comb[2]/4]\n",
    "\n",
    "# #For test:\n",
    "combinations = combinations1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten and KI:\n",
    "\n",
    "# Dados\n",
    "docs = df_data_ki['text']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "### início do loop:\n",
    "\n",
    "df = pd.DataFrame({'parameters': [],'n_neighbors': [], 'min_dist': [], 'min_cluster' : [], \n",
    "                      'min_sample': [], 'topic_coherence': [], 'nr_topics': [], 'avg_similarity' : [], 'topics': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    n_neighbors = comb[0]\n",
    "    min_dist = comb[1]\n",
    "    min_cluster = comb[2]\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, \n",
    "                        umap_model=umap_model, \n",
    "                        hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Matrix de similaridade:\n",
    "\n",
    "    matrixA = cosine_similarity(np.array(topic_model.topic_embeddings_)[1:, :])\n",
    "    avg_similarity = np.average(matrixA)\n",
    "    labels = (topic_model.get_topic_info().sort_values(\"Topic\", ascending=True).Name)[1:]\n",
    "    nr_topics = len(labels)\n",
    "    labels = labels.str.cat(sep='; ')\n",
    "    title = 'n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "    \n",
    "    #coherence score:\n",
    "    cs = calculuate_coherence_score(topic_model)\n",
    "\n",
    "    df_temp = pd.DataFrame({'parameters': [title],\n",
    "                            'n_neighbors': [n_neighbors],\n",
    "                            'min_dist': [min_dist],\n",
    "                            'min_cluster' : [min_cluster],\n",
    "                            'min_sample': [min_sample], \n",
    "                            'topic_coherence': [cs],\n",
    "                            'nr_topics': [nr_topics],\n",
    "                            'avg_similarity' : [avg_similarity],\n",
    "                            'topics': [labels]})\n",
    "    \n",
    "\n",
    "    pd.DataFrame({'parameters': [], \n",
    "                       'nr_topics': [], 'avg_similarity' : [], 'topics': []})\n",
    "\n",
    "\n",
    "    df = pd.concat([df, df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"df_avg_similarity_coerence_coeficient_daten_ki_eu.csv\", sep = ';', index = False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_avg_similarity_coerence_coeficient_daten_ki_eu.csv\", sep=\";\")\n",
    "df1 = df\n",
    "df1 = df1.sort_values('avg_similarity')\n",
    "df1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic coherence is a way to judge the quality of topics via a single quantitative, scalar value. There are many ways to compute the coherence score. For the u_mass and c_v options, a higher is always better. Note that u_mass is between -14 and 14 and c_v is between 0 and 1.\n",
    "\n",
    "https://datascience.oneoffcoder.com/topic-modeling-gensim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Plotando os gráficos de dispersão\n",
    "sns.scatterplot(x=\"nr_topics\", y=\"topic_coherence\", data=df, ax=axs[0][0])\n",
    "sns.scatterplot(x=\"avg_similarity\", y=\"topic_coherence\", data=df, ax=axs[0][1])\n",
    "sns.scatterplot(x=\"n_neighbors\", y=\"topic_coherence\", data=df, ax=axs[1][0])\n",
    "sns.scatterplot(x=\"min_dist\", y=\"topic_coherence\", data=df, ax=axs[1][1])\n",
    "\n",
    "# Adicionando títulos e rótulos de eixos\n",
    "axs[0][0].set_title(\"NR_Topics VS Topic Coherence\")\n",
    "axs[0][1].set_title(\"Avg_similarity VS Topic Coherence\")\n",
    "axs[1][0].set_title(\"N_neighbors VS Topic Coherence\")\n",
    "axs[1][1].set_title(\"Min_dist VS Topic Coherence\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a dispersao:\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# criando o gráfico de dispersão\n",
    "sns.scatterplot(data=df1, x='avg_similarity', y='topic_coherence', hue='nr_topics')\n",
    "\n",
    "# adicionando labels de eixo\n",
    "plt.xlabel('avg_similarity')\n",
    "plt.ylabel('topic_coherence')\n",
    "\n",
    "# adicionando título\n",
    "plt.title(\"EU Daten Und KI: coherence Vs avg_similarity\")\n",
    "\n",
    "# exibindo o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda parte: pegando exemplos dentro dos modelos escolhidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_avg_similarity_coerence_coeficient_daten_ki_eu.csv\", sep = \";\")\n",
    "#df.describe()\n",
    "# topic_coherence > 0,55 & avg_similarity < 0.85\n",
    "df1 = df.query('topic_coherence > 0.55 & avg_similarity < 0.85')\n",
    "df1 = df1.reset_index()\n",
    "\n",
    "combinations = []\n",
    "\n",
    "for i in range(0,len(df1)):\n",
    "    n = int(df1.iloc[i, 2])  # linha coluna\n",
    "    d = int(df1.iloc[i, 3])\n",
    "    c = int(df1.iloc[i, 4])\n",
    "    s = int(df1.iloc[i, 5])\n",
    "    \n",
    "    com = [(n, d, c, s)]\n",
    "\n",
    "    combinations.extend(com)\n",
    "\n",
    "# #Pra teste:\n",
    "# combinations = combinations[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten and KI:\n",
    "\n",
    "# Dados\n",
    "docs = df_data_ki['text']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "### início do loop:\n",
    "\n",
    "df = pd.DataFrame({'Document': [],'Topic': [], 'Name': [], 'Top_n_words' : [], 'Probability': [], 'Representative_document': [], 'model': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    n_neighbors = comb[0]\n",
    "    min_dist = comb[1]\n",
    "    min_cluster = comb[2]\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Dataframe with representative examples:\n",
    "\n",
    "    df_temp = topic_model.get_document_info(docs)\n",
    "    df_temp = df_temp.query('Representative_document == True')\n",
    "\n",
    "    #confirmando o modelo desse df\n",
    "    position = combinations.index(comb)\n",
    "    position = str(position)\n",
    "\n",
    "    #add modelo\n",
    "    df_temp['model'] = 'model ' + position\n",
    "\n",
    "    df = pd.concat([df, df_temp], axis=0)\n",
    "\n",
    "    ##### Visualizacao dos resultados\n",
    "\n",
    "    title = 'EU_Daten_ki_n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "\n",
    "    fig_1 = topic_model.visualize_heatmap()\n",
    "    fig_1.update_layout(title = title)\n",
    "\n",
    "    ############################################Visualizacao da distribuicao\n",
    "\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")   #Suitable Score Functions: cosine\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    # Train BERTopic\n",
    "    topic_model = BERTopic(language = \"german\", \n",
    "                            umap_model=umap_model,\n",
    "                            vectorizer_model=vectorizer_model,\n",
    "                            hdbscan_model=hdbscan_model).fit(docs, embeddings)\n",
    "\n",
    "\n",
    "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    fig_2 = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "    fig_2.update_layout(title = title)\n",
    "\n",
    "    title = re.sub(r'\\s', '_', title) + '.html'\n",
    "\n",
    "    with open(title, 'a') as f:\n",
    "        f.write(fig_1.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write(fig_2.to_html(full_html=False, include_plotlyjs='cdn'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_examples_models_daten_ki_eu.csv\", sep = ';', index = False, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc948d7f5534a86a8885c7bcf9c4eb48b42c573fd8ef9d73032ed45fcd97e814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
