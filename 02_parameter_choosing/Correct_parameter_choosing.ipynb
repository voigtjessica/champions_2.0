{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct_parameter_choosing\n",
    "\n",
    "Due to a problem in original scripts, the min_dist parameter was been reading as `int` and not `float` , and therefore for some groups of documents some visualizations were not made. The missing visualizations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Visualizations\n",
    "\n",
    "# EU - Daten und KI ## FEITO\n",
    "\n",
    "#  (5, 0.25, 30, 7),\n",
    "#  (10, 0.0, 10, 2), OK\n",
    "#  (10, 0.0, 15, 3), OK\n",
    "#  (10, 0.0, 20, 5), OK\n",
    "#  (10, 0.1, 10, 2),\n",
    "#  (10, 0.1, 15, 3),\n",
    "#  (10, 0.1, 20, 5),\n",
    "#  (10, 0.1, 25, 6),\n",
    "#  (10, 0.25, 10, 2),\n",
    "#  (10, 0.25, 20, 5),\n",
    "#  (10, 0.25, 25, 6),  \n",
    "#  (20, 0.0, 15, 3), OK\n",
    "#  (20, 0.1, 15, 3),\n",
    "#  (20, 0.1, 20, 5),\n",
    "#  (20, 0.25, 15, 3),\n",
    "#  (20, 0.25, 30, 7)\n",
    "\n",
    "\n",
    "# EU - Timber\n",
    "\n",
    "# (5.0, 0.0, 10, 2) OK\n",
    "# (7.0, 0.1, 30, 7)\n",
    "# (7.0, 0.25, 25, 6)\n",
    "# (7.0, 0.25, 30, 7)\n",
    "# (10.0, 0.0, 25, 6) OK\n",
    "# (10.0, 0.1, 25, 6)\n",
    "# (20.0, 0.25, 10, 2)\n",
    "\n",
    "# AT Daten KI\n",
    "\n",
    "# (7.0, 0.25, 30, 7) OK\n",
    "# (10.0, 0.0, 30, 7) OK\n",
    "# (10.0, 0.1, 25, 6) OK\n",
    "# (10.0, 0.25, 25, 6) OK\n",
    "# (20.0, 0.0, 25, 6) OK\n",
    "# (20.0, 0.0, 30, 7) OK\n",
    "# (20.0, 0.25, 25, 6) OK\n",
    "# (20.0, 0.25, 30, 7) OK\n",
    "\n",
    "# AT Regierung\n",
    "# (5.0, 0.25, 25, 6)\n",
    "# (7.0, 0.25, 20, 5)\n",
    "# (7.0, 0.25, 25, 6)\n",
    "# (7.0, 0.25, 30, 7)\n",
    "# (20.0, 0.0, 20, 5) OK\n",
    "\n",
    "# AT timber\n",
    "\n",
    "# (5.0, 0.0, 10, 2) OK\n",
    "# (5.0, 0.0, 15, 3) OK\n",
    "# (5.0, 0.1, 15, 3)\n",
    "# (5.0, 0.25, 10, 2)\n",
    "# (5.0, 0.25, 20, 5)\n",
    "# (7.0, 0.1, 10, 2)\n",
    "# (7.0, 0.25, 10, 2)\n",
    "# (10.0, 0.1, 10, 2)\n",
    "# (10.0, 0.25, 10, 2)\n",
    "# (10.0, 0.25, 15, 3)\n",
    "# (20.0, 0.1, 10, 2)\n",
    "# (20.0, 0.1, 15, 3)\n",
    "# (20.0, 0.25, 10, 2)\n",
    "# (20.0, 0.25, 15, 3)\n",
    "\n",
    "# DE Regierung\n",
    "\n",
    "# (5.0, 0.0, 15, 3) OK\n",
    "# (5.0, 0.1, 10, 2) OK\n",
    "# (5.0, 0.1, 15, 3) OK\n",
    "# (7.0, 0.0, 10, 2) OK\n",
    "\n",
    "# DE Timber\n",
    "\n",
    "# (7.0, 0.0, 15, 3) OK\n",
    "# (7.0, 0.0, 25, 6) OK\n",
    "# (7.0, 0.1, 20, 5) OK\n",
    "# (7.0, 0.25, 20, 5) OK\n",
    "# (7.0, 0.25, 25, 6) OK\n",
    "# (10.0, 0.0, 20, 5) OK\n",
    "# (10.0, 0.1, 25, 6) OK\n",
    "# (10.0, 0.25, 25, 6) OK\n",
    "# (20.0, 0.0, 25, 6) OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Importando tudo que já está fixo:\n",
    "\n",
    "# Data processing\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Topic model\n",
    "import umap\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "# Countvectorizer (para stopwords)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## For UMAP Analisys:\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# para montar as combinacoes:\n",
    "from itertools import product\n",
    "\n",
    "# Para a matrix \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Topic Coherence:\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "## Functions to clean data:\n",
    "\n",
    "def replace_char(x):\n",
    "  return re.sub('[^a-zA-Z\\süöäß]', '', x)\n",
    "\n",
    "\n",
    "def clean_corpus(s):\n",
    "    s = s.apply(lambda x: x.lower() if isinstance(x, str) else x)     # tolower:\n",
    "    idx = s.apply(lambda x: isinstance(x, str))                       # Criar um índice booleano para filtrar os valores não-texto\n",
    "    s = s.drop(idx[~idx].index)                                       # Remover os valores não-texto\n",
    "    s = s.apply(replace_char)                                         # remover as partes de palavras que nao sao caracteres ou espacos\n",
    "    idx = s.apply(lambda x: len(x) < 2)                               # Criar um índice booleano para filtrar os elementos com comprimento menor que 2\n",
    "    s = s.drop(idx[idx].index)                                        # Remover os elementos com comprimento menor que 2\n",
    "    idx = s.apply(lambda x: x == '')                                  # Criar um índice booleano para filtrar os elementos vazios\n",
    "    s = s.drop(idx[idx].index)                                        # Remover os elementos vazios\n",
    "    \n",
    "    return s\n",
    "\n",
    "def calculuate_coherence_score(topic_model):  \n",
    "  #variable\n",
    "  topic_words = topic_words = [[words for words, _ in topic_model.get_topic(topic) if words!=''] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "  vectorizer = topic_model.vectorizer_model\n",
    "  tokenizer = vectorizer.build_tokenizer()\n",
    "  #dictionary\n",
    "  tokens = [doc.split() for doc in docs]\n",
    "  dictionary = corpora.Dictionary(tokens)\n",
    "  corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "\n",
    "  coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "  coherence = coherence_model.get_coherence()\n",
    "\n",
    "  \n",
    "  return coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0.25, 30, 7)\n",
      "(10, 0.1, 10, 2)\n",
      "(10, 0.1, 15, 3)\n",
      "(10, 0.1, 20, 5)\n",
      "(10, 0.1, 25, 6)\n",
      "(10, 0.25, 10, 2)\n",
      "(10, 0.25, 20, 5)\n",
      "(10, 0.25, 25, 6)\n",
      "(20, 0.1, 15, 3)\n",
      "(20, 0.1, 20, 5)\n",
      "(20, 0.25, 15, 3)\n",
      "(20, 0.25, 30, 7)\n"
     ]
    }
   ],
   "source": [
    "# EU - Daten und KI\n",
    "\n",
    "combinations = [ (5, 0.25, 30, 7), (10, 0.1, 10, 2),  (10, 0.1, 15, 3),  (10, 0.1, 20, 5),  (10, 0.1, 25, 6),  (10, 0.25, 10, 2),  (10, 0.25, 20, 5),\n",
    "                (10, 0.25, 25, 6),  (20, 0.1, 15, 3), (20, 0.1, 20, 5),  (20, 0.25, 15, 3),  (20, 0.25, 30, 7)]\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_original = pd.read_csv(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\df_data_ki.csv\")\n",
    "\n",
    "\n",
    "# German stopwords que eu alterei:\n",
    "# Carregue a lista a partir do arquivo\n",
    "with open(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\german_stopwords.json\", \"r\") as f:\n",
    "    german_stopwords_alterada = json.load(f) \n",
    "\n",
    "df_work = df_original\n",
    "\n",
    "# nome dos dados:\n",
    "tinicio = 'eu_daten_ki_'\n",
    "\n",
    "docs = df_original['text']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "\n",
    "\n",
    "###### inicio do loop #######\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({'Document': [],'Topic': [], 'Name': [], 'Top_n_words' : [], 'Probability': [], 'Representative_document': [], 'model': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    print(comb)\n",
    "\n",
    "\n",
    "    n_neighbors = int(comb[0])\n",
    "    min_dist = float(comb[1])\n",
    "    min_cluster = int(comb[2])\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Dataframe with representative examples:\n",
    "\n",
    "    df_temp = topic_model.get_document_info(docs)\n",
    "    df_temp = df_temp.query('Representative_document == True')\n",
    "\n",
    "    #confirmando o modelo desse df\n",
    "    position = combinations.index(comb)\n",
    "    position = str(position)\n",
    "\n",
    "    #add modelo\n",
    "    df_temp['model'] =  str(comb)\n",
    "\n",
    "    df2 = pd.concat([df2, df_temp], axis=0)\n",
    "\n",
    "        ##### Visualizacao dos resultados\n",
    "\n",
    "    title = tinicio + '_n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "\n",
    "    fig_1 = topic_model.visualize_heatmap()\n",
    "    fig_1.update_layout(title = title)\n",
    "\n",
    "    ############################################Visualizacao da distribuicao\n",
    "\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")   #Suitable Score Functions: cosine\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    # Train BERTopic\n",
    "    topic_model = BERTopic(language = \"german\", \n",
    "                            umap_model=umap_model,\n",
    "                            vectorizer_model=vectorizer_model,\n",
    "                            hdbscan_model=hdbscan_model).fit(docs, embeddings)\n",
    "\n",
    "\n",
    "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    fig_2 = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "    fig_2.update_layout(title = title)\n",
    "\n",
    "    title = re.sub(r'\\s', '_', title) + '.html'\n",
    "\n",
    "    with open(title, 'a') as f:\n",
    "        f.write(fig_1.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write(fig_2.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "#Fora do loop:\n",
    "\n",
    "tituloex_models = 'extra_df_examples_models_' + tinicio + '.csv'\n",
    "df2.to_csv(tituloex_models, sep = ';', index = False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7.0, 0.1, 30, 7)\n",
      "(7.0, 0.25, 25, 6)\n",
      "(7.0, 0.25, 30, 7)\n",
      "(10.0, 0.1, 25, 6)\n",
      "(20.0, 0.25, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "# EU - Timber\n",
    "\n",
    "combinations = [(7.0, 0.1, 30, 7), (7.0, 0.25, 25, 6), (7.0, 0.25, 30, 7), (10.0, 0.1, 25, 6), (20.0, 0.25, 10, 2)]\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_clima = pd.read_csv(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champI4.0ns\\\\gesetze_sammlung\\\\df_clima.csv\")\n",
    "df_timber = pd.read_csv(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champI4.0ns\\\\gesetze_sammlung\\\\df_timber.csv\")\n",
    "\n",
    "# Ajeitando os dados:\n",
    "\n",
    "df_original = pd.concat([df_clima, df_timber])\n",
    "\n",
    "# German stopwords que eu alterei:\n",
    "# Carregue a lista a partir do arquivo\n",
    "with open(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\german_stopwords.json\", \"r\") as f:\n",
    "    german_stopwords_alterada = json.load(f) \n",
    "\n",
    "df_work = df_original\n",
    "\n",
    "# nome dos dados:\n",
    "tinicio = 'eu_timber_'\n",
    "\n",
    "docs = df_work['text']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "\n",
    "\n",
    "###### inicio do loop #######\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({'Document': [],'Topic': [], 'Name': [], 'Top_n_words' : [], 'Probability': [], 'Representative_document': [], 'model': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    print(comb)\n",
    "\n",
    "\n",
    "    n_neighbors = int(comb[0])\n",
    "    min_dist = float(comb[1])\n",
    "    min_cluster = int(comb[2])\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Dataframe with representative examples:\n",
    "\n",
    "    df_temp = topic_model.get_document_info(docs)\n",
    "    df_temp = df_temp.query('Representative_document == True')\n",
    "\n",
    "    #confirmando o modelo desse df\n",
    "    position = combinations.index(comb)\n",
    "    position = str(position)\n",
    "\n",
    "    #add modelo\n",
    "    df_temp['model'] =  str(comb)\n",
    "\n",
    "    df2 = pd.concat([df2, df_temp], axis=0)\n",
    "\n",
    "        ##### Visualizacao dos resultados\n",
    "\n",
    "    title = tinicio + '_n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "\n",
    "    fig_1 = topic_model.visualize_heatmap()\n",
    "    fig_1.update_layout(title = title)\n",
    "\n",
    "    ############################################Visualizacao da distribuicao\n",
    "\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")   #Suitable Score Functions: cosine\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    # Train BERTopic\n",
    "    topic_model = BERTopic(language = \"german\", \n",
    "                            umap_model=umap_model,\n",
    "                            vectorizer_model=vectorizer_model,\n",
    "                            hdbscan_model=hdbscan_model).fit(docs, embeddings)\n",
    "\n",
    "\n",
    "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    fig_2 = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "    fig_2.update_layout(title = title)\n",
    "\n",
    "    title = re.sub(r'\\s', '_', title) + '.html'\n",
    "\n",
    "    with open(title, 'a') as f:\n",
    "        f.write(fig_1.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write(fig_2.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "#Fora do loop:\n",
    "\n",
    "tituloex_models = 'extra_df_examples_models_' + tinicio + '.csv'\n",
    "df2.to_csv(tituloex_models, sep = ';', index = False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Continuar daqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# nome dos dados:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m tinicio \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mat_regierung_\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 19\u001b[0m docs \u001b[39m=\u001b[39m df_work[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     20\u001b[0m docs \u001b[39m=\u001b[39m clean_corpus(docs)\u001b[39m.\u001b[39mto_list()\n\u001b[0;32m     24\u001b[0m \u001b[39m###### inicio do loop #######\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# AT Regierung\n",
    "combinations = [(5.0, 0.25, 25, 6), (7.0, 0.25, 20, 5), (7.0, 0.25, 25, 6), (7.0, 0.25, 30, 7)]\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_original = pd.read_json(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\at_concatenado.json\")\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_work = df_original.query(\"documento == 'regierungsprogramm'\")\n",
    "\n",
    "# Austrian stopwords que eu alterei:\n",
    "# Carregue a lista a partir do arquivo\n",
    "with open(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\austrian_stopwords.json\", \"r\") as f:\n",
    "    german_stopwords_alterada = json.load(f) \n",
    "\n",
    "\n",
    "# nome dos dados:\n",
    "tinicio = 'at_regierung_'\n",
    "\n",
    "docs = df_work['text']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "\n",
    "\n",
    "###### inicio do loop #######\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({'Document': [],'Topic': [], 'Name': [], 'Top_n_words' : [], 'Probability': [], 'Representative_document': [], 'model': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    print(comb)\n",
    "\n",
    "\n",
    "    n_neighbors = int(comb[0])\n",
    "    min_dist = float(comb[1])\n",
    "    min_cluster = int(comb[2])\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Dataframe with representative examples:\n",
    "\n",
    "    df_temp = topic_model.get_document_info(docs)\n",
    "    df_temp = df_temp.query('Representative_document == True')\n",
    "\n",
    "    #confirmando o modelo desse df\n",
    "    position = combinations.index(comb)\n",
    "    position = str(position)\n",
    "\n",
    "    #add modelo\n",
    "    df_temp['model'] =  str(comb)\n",
    "\n",
    "    df2 = pd.concat([df2, df_temp], axis=0)\n",
    "\n",
    "        ##### Visualizacao dos resultados\n",
    "\n",
    "    title = tinicio + '_n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "\n",
    "    fig_1 = topic_model.visualize_heatmap()\n",
    "    fig_1.update_layout(title = title)\n",
    "\n",
    "    ############################################Visualizacao da distribuicao\n",
    "\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")   #Suitable Score Functions: cosine\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    # Train BERTopic\n",
    "    topic_model = BERTopic(language = \"german\", \n",
    "                            umap_model=umap_model,\n",
    "                            vectorizer_model=vectorizer_model,\n",
    "                            hdbscan_model=hdbscan_model).fit(docs, embeddings)\n",
    "\n",
    "\n",
    "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    fig_2 = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "    fig_2.update_layout(title = title)\n",
    "\n",
    "    title = re.sub(r'\\s', '_', title) + '.html'\n",
    "\n",
    "    with open(title, 'a') as f:\n",
    "        f.write(fig_1.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write(fig_2.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "#Fora do loop:\n",
    "\n",
    "tituloex_models = 'extra_df_examples_models_' + tinicio + '.csv'\n",
    "df2.to_csv(tituloex_models, sep = ';', index = False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AT timber\n",
    "\n",
    "combinations = [(5.0, 0.1, 15, 3), (5.0, 0.25, 10, 2) , (5.0, 0.25, 20, 5), (7.0, 0.1, 10, 2), (7.0, 0.25, 10, 2) , (10.0, 0.1, 10, 2) , (10.0, 0.25, 10, 2), (10.0, 0.25, 15, 3), (20.0, 0.1, 10, 2), (20.0, 0.1, 15, 3), (20.0, 0.25, 10, 2), (20.0, 0.25, 15, 3)]\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_original = pd.read_json(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\at_concatenado.json\")\n",
    "\n",
    "# Abrindo o arquivos\n",
    "df_work = df_original.query(\"documento == 'oestereiche_holzinitiative' | documento == 'nationale_strategie_kreislaufwirtschaft' \")\n",
    "\n",
    "# Austrian stopwords que eu alterei:\n",
    "# Carregue a lista a partir do arquivo\n",
    "with open(\"C:\\\\Users\\\\JVoigt\\\\OneDrive - Universität für Weiterbildung Krems\\\\Dokumente\\\\Python Scripts\\\\champions_2.0\\\\01_gesetze_sammlung\\\\austrian_stopwords.json\", \"r\") as f:\n",
    "    german_stopwords_alterada = json.load(f) \n",
    "\n",
    "\n",
    "# nome dos dados:\n",
    "tinicio = 'at_timber_'\n",
    "\n",
    "docs = df_work['text']\n",
    "docs = clean_corpus(docs).to_list()\n",
    "\n",
    "\n",
    "\n",
    "###### inicio do loop #######\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame({'Document': [],'Topic': [], 'Name': [], 'Top_n_words' : [], 'Probability': [], 'Representative_document': [], 'model': []})\n",
    "\n",
    "for comb in combinations:\n",
    "    print(comb)\n",
    "\n",
    "\n",
    "    n_neighbors = int(comb[0])\n",
    "    min_dist = float(comb[1])\n",
    "    min_cluster = int(comb[2])\n",
    "    min_sample = int(comb[3])\n",
    "\n",
    "\n",
    "    umap_model  = UMAP(n_neighbors=n_neighbors, \n",
    "                            min_dist = min_dist,\n",
    "                            n_components=5, \n",
    "                            metric='cosine', random_state=42)\n",
    "\n",
    "\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, \n",
    "                            min_samples = min_sample, \n",
    "                            metric='euclidean', \n",
    "                            prediction_data=True)\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=german_stopwords_alterada) #german_stop_words alterada\n",
    "\n",
    "    topic_model = BERTopic(language = \"german\", vectorizer_model=vectorizer_model, umap_model=umap_model, hdbscan_model=hdbscan_model).fit(docs)\n",
    "\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Dataframe with representative examples:\n",
    "\n",
    "    df_temp = topic_model.get_document_info(docs)\n",
    "    df_temp = df_temp.query('Representative_document == True')\n",
    "\n",
    "    #confirmando o modelo desse df\n",
    "    position = combinations.index(comb)\n",
    "    position = str(position)\n",
    "\n",
    "    #add modelo\n",
    "    df_temp['model'] =  str(comb)\n",
    "\n",
    "    df2 = pd.concat([df2, df_temp], axis=0)\n",
    "\n",
    "        ##### Visualizacao dos resultados\n",
    "\n",
    "    title = tinicio + '_n_neighbors = ' + str(n_neighbors) + ' min_dist = ' + str(min_dist) + ' min_cluster = ' + str(min_cluster) + ' min_sample = ' + str(min_sample)\n",
    "\n",
    "    fig_1 = topic_model.visualize_heatmap()\n",
    "    fig_1.update_layout(title = title)\n",
    "\n",
    "    ############################################Visualizacao da distribuicao\n",
    "\n",
    "    sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")   #Suitable Score Functions: cosine\n",
    "    embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "\n",
    "    # Train BERTopic\n",
    "    topic_model = BERTopic(language = \"german\", \n",
    "                            umap_model=umap_model,\n",
    "                            vectorizer_model=vectorizer_model,\n",
    "                            hdbscan_model=hdbscan_model).fit(docs, embeddings)\n",
    "\n",
    "\n",
    "    # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "    fig_2 = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "    fig_2.update_layout(title = title)\n",
    "\n",
    "    title = re.sub(r'\\s', '_', title) + '.html'\n",
    "\n",
    "    with open(title, 'a') as f:\n",
    "        f.write(fig_1.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "        f.write(fig_2.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "#Fora do loop:\n",
    "\n",
    "tituloex_models = 'extra_df_examples_models_' + tinicio + '.csv'\n",
    "df2.to_csv(tituloex_models, sep = ';', index = False, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
