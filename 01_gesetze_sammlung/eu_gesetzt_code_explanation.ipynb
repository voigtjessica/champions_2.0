{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU Gesetzt Code explanation\n",
    "\n",
    "Tis is an explanation for myself to show how I builded the code. \n",
    "\n",
    "tutorial requests: https://medium.com/technofunnel/web-scraping-with-python-using-beautifulsoup-76b710e3e92f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32022R0868&qid=1666873096926&from=EN\"\n",
    "r = requests.get(URL) \n",
    "#print(r.content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the requests library, we’re able to extract the HTML associated with the website. The output received needs to be further evaluated before we can start extracting data from it. Currently, the output received is of the type string.\n",
    "\n",
    " Para parsear para uma estrutura tree-based, precisaremos usar a Beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html5lib') \n",
    "print(soup.prettify()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O texto que precede o início da lei é \"HABEN FOLGENDE VERORDNUNG ERLASSEN:\"\n",
    "\n",
    "The ```<div>``` tag defines a division or a section in an HTML document.\n",
    "\n",
    "No artigo um, temos a estrurura: Artigo >> Absatz >> Buchstaben (para alguns Absatz)\n",
    "No artigo dois, temos a estrutura: Artigo >> Nummer >> Buchstaben (para alguns Nummer)\n",
    "\n",
    "Entao:\n",
    "\n",
    "- No artigo 1, teremos 5 documentos (5 Absätze)\n",
    "- No artigo 2, teremos 1 documento (pois nao há divisao em Absatz)\n",
    "\n",
    "--\n",
    "\n",
    "Olhando o código, pude perceber que o id da DIV vai mudando conforme vamos navegando no artigo e no Absatz: ```<div id=\"001\">``` é Artigo 1 e ```<div id=\"001.001\">``` significa artigo 1, Absatz 1 e assim por diante. Quando o Artigo vai direto para os nummers, nao tem subdivisao da div ```<div id=\"002\"> ``` é o Artigo 2 e pronto. \n",
    "\n",
    "Exemplos dos códigos:\n",
    "\n",
    "1. Comeco do capítulo\n",
    "\n",
    "```\n",
    "    <p class=\"oj-ti-section-1\" id=\"d1e887-1-1\">\n",
    "    <span class=\"oj-italic\">\n",
    "     KAPITEL I\n",
    "```\n",
    "\n",
    "2. Comeco do Artigo:\n",
    "\n",
    "```\n",
    "    <div id=\"001\"> \n",
    "    <p class=\"oj-ti-art\" id=\"d1e897-1-1\">\n",
    "     Artikel 1\n",
    "    </p>\n",
    "    <p class=\"oj-sti-art\">\n",
    "     Gegenstand und Anwendungsbereich\n",
    "    </p>\n",
    "```\n",
    "\n",
    "3. Comeco do Absatz:\n",
    "\n",
    "```\n",
    "    <div id=\"001.001\">\n",
    "     <p class=\"oj-normal\">\n",
    "      (1)   In dieser Verordnung wird Folgendes festgelegt:\n",
    "     </p>\n",
    "```\n",
    "\n",
    "Absatz 2 do mesmo Artigo:\n",
    "\n",
    "```\n",
    "    <div id=\"001.002\">\n",
    "        <p class=\"oj-normal\">\n",
    "        (2)   Diese Verordnung begründet weder eine Verpflichtung für öffentliche Stellen, die Weiterverwendung von Daten zu erlauben, noch befreit sie öffentliche Stellen von ihren Geheimhaltungspflichten nach dem Unionsrecht oder dem nationalen Recht.\n",
    "        </p>\n",
    "```\n",
    "\n",
    "O que eu quero:\n",
    "\n",
    "- coletar todos os artigos separadamente\n",
    "- dentro dos artigos, separar por Absatz (quando tiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código inteiro:\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "URL = \"https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32022R0868&qid=1666873096926&from=EN\"\n",
    "r = requests.get(URL) \n",
    "soup = BeautifulSoup(r.content, 'html5lib') \n",
    "\n",
    "# Printa primeiro artigo\n",
    "table = soup.find('div', attrs = {'id':'001.001'}) \n",
    "\n",
    "for row in table:\n",
    "    print(row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código inteiro\n",
    "\n",
    "#import unicodedata\n",
    "#import pandas as pd\n",
    "\n",
    "\n",
    "# colocando em um dataframe:\n",
    "\n",
    "# Printa primeiro artigo\n",
    "\n",
    "df_law = pd.DataFrame(columns = ['artikel', 'absatz', 'text'])\n",
    "\n",
    "id_num= '001.001'\n",
    "table = soup.find('div', attrs = {'id':id_num}) \n",
    "\n",
    "my_str = table.text\n",
    "result = unicodedata.normalize('NFKD', my_str)   # removind \\xa0\n",
    "result = \" \".join(result.split())\n",
    "#result # lindo, está o absatz completo\n",
    "df_temp = pd.DataFrame({'artikel': [id_num]\n",
    "                        'absatz' : [id_num],\n",
    "                        'text': [result]})\n",
    "\n",
    "#df_temp.dtypes funciona\n",
    "df_law = pd.concat([df_law, df_temp])\n",
    "df_law\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mas eu quero descobrir todos os ids:\n",
    "\n",
    "ids = [tag['id'] for tag in soup.select('div[id]')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacao:  \" in your soup id is an attribute not a type of tag. To get a list of all id's in the soup you can use the following one liner (above) this uses CSS selectors instead of bs4's find_all since I find bs4's docs regarding its built-ins lacking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que eu quero fazer:\n",
    "\n",
    "1. Descobrir se ID tem absatz \n",
    "2. Se id tem Absatz - buscar absatz\n",
    "3. Se Id nao tem Absatz - Buscar ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'025.003', '029.003', '024.002', '011.007', '024.005', '004.004', '011.014', '011.013', '011.004', '005.008', '005.011', '006.004', '005.007', '030', '014.004', '006.005', '004.006', '011.010', '001.002', '014.003', '019.004', '013.002', '013.003', '001.004', '004.005', '014.002', '011.009', '026.004', '012', '006.003', '032.004', '019.003', '029.001', '006.002', '004.002', '009.001', '019.005', '026.003', '005.003', '007.001', '018', '028.003', '032.005', '032.006', '028.002', '021.004', '038', '021.002', '025.001', '034.002', '024.004', '007.003', '011.006', '024.001', '014.006', '008.002', '019.006', '031.001', '010', '029.002', '023.002', '019.007', '027.002', '002', '008.001', '026.005', '029.004', '001.001', '022.002', '021.003', '011.001', '011.012', '014.007', '026.001', '007.005', '006.006', '021.006', '023.001', '005.004', '014.001', '005.010', '017.001', '022.001', '011.011', '026.006', '025.004', '032.002', '011.002', '024.006', '032.003', '019.001', '027.001', '025.002', '037', '004.001', '019.002', '020.002', '031.002', '008.004', '035', '007.002', '031.003', '005.001', '031.004', '024.003', '015', '013.001', '021.001', '003.001', '036', '017.002', '026.002', '023.003', '005.006', '031.005', '021.005', '016', '034.001', '011.005', '005.005', '007.004', '033.003', '032.001', '006.001', '014.005', '005.009', '004.003', '028.001', '001.003', '008.003', '011.003', '005.014', '005.002', '001.005', '033.001', '009.002', '003.002', '005.013', '033.002', '020.001', '011.008', '005.012', '003.003'}\n"
     ]
    }
   ],
   "source": [
    "#Código final\n",
    "\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "ids = [tag['id'] for tag in soup.select('div[id]')]\n",
    "\n",
    "df = pd.DataFrame(ids)\n",
    "df.rename(columns={list(df)[0]:'ids'}, inplace=True) # renomeando baseado na posicao\n",
    "df['artikel'] = df['ids'].str[:3]\n",
    "df\n",
    "\n",
    "# Achando qtde de Absätze:\n",
    "\n",
    "df1 = df.groupby(['artikel']).count().reset_index()\n",
    "df1 = df1.query(\"ids > 1\")\n",
    "ids_mit = df1['artikel']#.astype('str')\n",
    "\n",
    "ids_final = set(ids) - set(ids_mit)\n",
    "\n",
    "print(ids_final)   # conferi o número, está correto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artikel</th>\n",
       "      <th>absatz</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>025</td>\n",
       "      <td>003</td>\n",
       "      <td>(3) Werden personenbezogene Daten erfasst, so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029</td>\n",
       "      <td>003</td>\n",
       "      <td>(3) Die Kommission führt den Vorsitz in den S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>024</td>\n",
       "      <td>002</td>\n",
       "      <td>(2) Die für die Registrierung von datenaltrui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038</td>\n",
       "      <td>NA</td>\n",
       "      <td>Artikel 38 Inkrafttreten und Geltung Diese Ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artikel absatz                                               text\n",
       "0     025    003  (3) Werden personenbezogene Daten erfasst, so ...\n",
       "0     029    003  (3) Die Kommission führt den Vorsitz in den S...\n",
       "0     024    002  (2) Die für die Registrierung von datenaltrui...\n",
       "0     038     NA  Artikel 38 Inkrafttreten und Geltung Diese Ver..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazendo o loop final para pegar todos os IDs com e sem Absatze:\n",
    "\n",
    "ids_teste = ['025.003', '029.003', '024.002','038']\n",
    "\n",
    "# colocando em um dataframe:\n",
    "df_law = pd.DataFrame(columns = ['artikel', 'absatz', 'text'])\n",
    "\n",
    "for id_entry in ids_teste:\n",
    "    #pegando tudo:\n",
    "    table = soup.find('div', attrs = {'id':id_entry}) \n",
    "    my_str = table.text\n",
    "    result = unicodedata.normalize('NFKD', my_str)   # removind \\xa0\n",
    "    result = \" \".join(result.split())\n",
    "    #result # lindo, está o absatz completo\n",
    "    #agora pegando os artigos e os absatz\n",
    "    #criando um df temporario com os detalhes do artigo:\n",
    "    if len(id_entry) < 4:\n",
    "        df_temp = pd.DataFrame({'artikel': [id_entry.split('.')[0]], \n",
    "                                'absatz' : [\"NA\"],\n",
    "                                'text': [result]})\n",
    "    else:\n",
    "        df_temp = pd.DataFrame({'artikel': [id_entry.split('.')[0]], \n",
    "                                'absatz' : [id_entry.split('.')[1]],\n",
    "                                'text': [result]})\n",
    "    #df_temp.dtypes funciona\n",
    "    df_law = pd.concat([df_law, df_temp])\n",
    "\n",
    "df_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#### CÓDIGO FINAL: ####\n",
    "\n",
    "\n",
    "## Getting the law:\n",
    "\n",
    "URL = \"https://eur-lex.europa.eu/legal-content/DE/TXT/HTML/?uri=CELEX:32022R0868&qid=1666873096926&from=EN\"\n",
    "r = requests.get(URL) \n",
    "soup = BeautifulSoup(r.content, 'html5lib') \n",
    "\n",
    "\n",
    "## Getting all the artikels and separating Artikels with and without Absatze:\n",
    "\n",
    "#1. getting all artikels and absatze:\n",
    "\n",
    "ids = [tag['id'] for tag in soup.select('div[id]')]\n",
    "\n",
    "df = pd.DataFrame(ids)\n",
    "df.rename(columns={list(df)[0]:'ids'}, inplace=True)\n",
    "df['artikel'] = df['ids'].str[:3]\n",
    "df\n",
    "\n",
    "#2. Looking which one of them have Absatze\n",
    "\n",
    "df1 = df.groupby(['artikel']).count().reset_index()\n",
    "df1 = df1.query(\"ids > 1\")\n",
    "ids_mit = df1['artikel']#.astype('str')\n",
    "\n",
    "#3. Getting the final ids, so the ids and ids with absatze dont appear twice:\n",
    "ids_final = set(ids) - set(ids_mit)\n",
    "\n",
    "## Creating the table with all Artikel and Absatze:\n",
    "\n",
    "#1. Creating empty df:\n",
    "df_law = pd.DataFrame(columns = ['artikel', 'absatz', 'text'])\n",
    "\n",
    "#2. loop for ids:\n",
    "\n",
    "for id_entry in ids_final:\n",
    "    #pegando tudo:\n",
    "    table = soup.find('div', attrs = {'id':id_entry}) \n",
    "    my_str = table.text\n",
    "    result = unicodedata.normalize('NFKD', my_str)   # removind \\xa0\n",
    "    result = \" \".join(result.split())\n",
    "    #result # lindo, está o absatz completo\n",
    "    #agora pegando os artigos e os absatz\n",
    "    #criando um df temporario com os detalhes do artigo:\n",
    "    if len(id_entry) < 4:\n",
    "        df_temp = pd.DataFrame({'artikel': [id_entry.split('.')[0]], \n",
    "                                'absatz' : [\"NA\"],\n",
    "                                'text': [result]})\n",
    "    else:\n",
    "        df_temp = pd.DataFrame({'artikel': [id_entry.split('.')[0]], \n",
    "                                'absatz' : [id_entry.split('.')[1]],\n",
    "                                'text': [result]})\n",
    "    #df_temp.dtypes funciona\n",
    "    df_law = pd.concat([df_law, df_temp])\n",
    "\n",
    "df_law\n",
    "\n",
    "df_law.to_csv(\"df_law_teste.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
